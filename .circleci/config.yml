version: 2.1

parameters:
  GHA_Actor:
    type: string
    default: ""
  GHA_Action:
    type: string
    default: ""
  GHA_Event:
    type: string
    default: ""
  GHA_Meta:
    type: string
    default: ""

orbs:
  aws-s3: circleci/aws-s3@3.0
  aws-cli: circleci/aws-cli@5.4.1

jobs:
  task-containerisation:
    parameters:
      aws_account_id:
        type: env_var_name
      access_key:
        type: env_var_name
      secret_key:
        type: env_var_name
    docker:
      - image: docker:latest
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - aws-cli/setup:
          region: "eu-west-2"
      - run:
          name: Build Docker Images
          command: |
            export AWS_ACCESS_KEY_ID=${<< parameters.access_key >>}
            export AWS_SECRET_ACCESS_KEY=${<< parameters.secret_key >>}
            # Authenticate Docker to ECR
            aws ecr get-login-password --region eu-west-2 | docker login -u AWS --password-stdin ${<< parameters.aws_account_id >>}.dkr.ecr.eu-west-2.amazonaws.com
            echo "Rebuilding lambdas and pushing to ECR."
            export AWS_ACCOUNT_ID=${<< parameters.aws_account_id >>}
            docker buildx bake all --push

  test:
    docker:
      - image: cimg/python:3.11.12
    resource_class: large
    steps:
      - checkout
      - restore_cache:
          keys:
            - v1.1-deps-{{ checksum "Pipfile.lock" }}
      - run:
          name: Install system packages
          command: |
            sudo apt-get update
            sudo apt-get install -y default-jre-headless
      - run:
          name: install dependencies
          command: |
            pip install pipenv
            pipenv install --dev
      - save_cache:
          key: v1.1-deps-{{ checksum "Pipfile.lock" }}
          paths:
            - ~/.local/share/virtualenvs
      - run:
          name: run unit tests
          command: |
            pipenv run coverage run --source=. -m unittest discover -s . -p "test_*.py"
      - run:
          name: Generate coverage report
          command: |
            pipenv run coverage report
            pipenv run coverage xml
            pipenv run coverage html
      - run:
          name: run integration tests
          command: |
            pipenv run python -m unittest discover tests/integration
      - store_artifacts:
          path: htmlcov

  lint-python:
    docker:
      - image: cimg/python:3.11.12
    resource_class: small
    steps:
      - checkout
      - run:
          name: Install black
          command: pip install black==25.1.0
      - run:
          name: Check Python formatting
          command: black . --check

  lint-docstrings:
    docker:
      - image: cimg/python:3.11.12
    resource_class: small
    steps:
      - checkout
      - run:
          name: Install pydoclint
          command: pip install pydoclint==0.6.10
      - run:
          name: Check docstring formatting
          command: pydoclint --style=google --quiet .

  lint-terraform:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.13.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: Check terraform formatting
          command: terraform fmt -check -recursive -diff

  terraform-plan:
    parameters:
      aws_account:
        type: enum
        description: The short name of the AWS account, used to reference the relevant `tfbackend` file, and auth credentials
        enum: ["prod", "non_prod"]
      access_key:
        type: env_var_name
      secret_key:
        type: env_var_name
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.13.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: terraform plan
          command: |
            export AWS_ACCESS_KEY_ID=${<< parameters.access_key >>}
            export AWS_SECRET_ACCESS_KEY=${<< parameters.secret_key >>}
            cd terraform/pipeline
            terraform init -input=false -backend-config=../<< parameters.aws_account >>.s3.tfbackend
            terraform workspace select ${CIRCLE_BRANCH} || terraform workspace new ${CIRCLE_BRANCH}
            terraform plan -out tfapply -var-file=../<< parameters.aws_account >>.s3.tfbackend
      - store_artifacts:
          path: terraform/pipeline/tfapply
      - persist_to_workspace:
          root: .
          paths:
            - terraform/pipeline/tfapply
            - terraform/pipeline/.*
            - .

  terraform-apply:
    parameters:
      aws_account:
        type: enum
        description: The short name of the AWS account, used to reference the relevant auth credentials
        enum: ["prod", "non_prod"]
      access_key:
        type: env_var_name
      secret_key:
        type: env_var_name
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.13.1
    resource_class: small
    steps:
      - checkout
      - attach_workspace:
          at: .
      - run:
          name: terraform apply
          command: |
            export AWS_ACCESS_KEY_ID=${<< parameters.access_key >>}
            export AWS_SECRET_ACCESS_KEY=${<< parameters.secret_key >>}
            cd terraform/pipeline
            terraform apply -auto-approve tfapply
      - run:
          name: save pipeline resources bucket name
          command: |
            cd terraform/pipeline
            echo $(terraform output --raw pipeline_resources_bucket_name) > pipeline_resources_bucket_name
      - run:
          name: save datasets bucket name
          command: |
            cd terraform/pipeline
            echo $(terraform output --raw datasets_bucket_name) > datasets_bucket_name
      - persist_to_workspace:
          root: .
          paths:
            - ./terraform/pipeline/pipeline_resources_bucket_name
            - ./terraform/pipeline/datasets_bucket_name

  deploy-dependencies:
    parameters:
      access_key:
        type: env_var_name
      secret_key:
        type: env_var_name
    docker:
      - image: cimg/python:3.11.12
    resource_class: small
    steps:
      - attach_workspace:
          at: .
      - run:
          name: package utils, schemas, projects and environment
          command: |
            mkdir dependencies && zip -r dependencies/dependencies.zip utils schemas projects environment
      - run:
          name: Download Deequ jar
          command: |
            wget https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.8-spark-3.5/deequ-2.0.8-spark-3.5.jar -O dependencies/deequ-2.0.8-spark-3.5.jar
      - run:
          name: Package PyDeequ
          command: |
            pip install -t ./target pydeequ==1.5.0
            cd target && zip -r pydeequ-1.5.0.zip pydeequ && cd ..
            mv target/pydeequ-1.5.0.zip dependencies/pydeequ-1.5.0.zip
            rm -rf target
      - aws-s3/sync:
          aws-region: AWS_REGION
          from: dependencies
          to: "s3://$(cat terraform/pipeline/pipeline_resources_bucket_name)/dependencies"

  copy-default-data:
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID_non_prod}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY_non_prod}
    docker:
      - image: cimg/base:current
    resource_class: small
    steps:
      - attach_workspace:
          at: .
      - aws-s3/sync:
          aws-region: AWS_REGION
          from: "s3://sfc-default-datasets/"
          to: "s3://$(cat terraform/pipeline/datasets_bucket_name)/"
      - aws-s3/sync:
          aws-region: AWS_REGION
          from: "s3://sfc-main-pipeline-resources/models/"
          to: "s3://$(cat terraform/pipeline/pipeline_resources_bucket_name)/models/"

  terraform-destroy:
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID_non_prod}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY_non_prod}
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.13.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: terraform destroy
          command: |
            cd terraform/pipeline
            terraform init -input=false -backend-config=../<< parameters.aws_account >>.s3.tfbackend
            terraform workspace select << pipeline.parameters.GHA_Meta >>
            terraform destroy -auto-approve -var-file=../<< parameters.aws_account >>.s3.tfbackend
      - run:
          name: delete workspace
          command: |
            cd terraform/pipeline
            terraform workspace select default
            terraform workspace delete << pipeline.parameters.GHA_Meta >>


workflows:
  version: 2
  test-plan-approve-and-deploy-to-main:
    unless: << pipeline.parameters.GHA_Action >>
    jobs:
      - task-containerisation:
          access_key: AWS_ACCESS_KEY_ID_prod
          secret_key: AWS_SECRET_ACCESS_KEY_prod
          aws_account_id: AWS_ACCOUNT_ID_prod
          filters:
            branches:
              only: main
      - test:
          filters:
            branches:
              only: main
      - terraform-plan:
          access_key: AWS_ACCESS_KEY_ID_prod
          secret_key: AWS_SECRET_ACCESS_KEY_prod
          aws_account: "prod"
          requires:
            - test
            - task-containerisation
      - plan-approval:
          type: approval
          requires:
            - terraform-plan
      - terraform-apply:
          access_key: AWS_ACCESS_KEY_ID_prod
          secret_key: AWS_SECRET_ACCESS_KEY_prod
          aws_account: "prod"
          requires:
            - plan-approval
      - deploy-dependencies:
          access_key: AWS_ACCESS_KEY_ID_prod
          secret_key: AWS_SECRET_ACCESS_KEY_prod
          requires:
            - terraform-apply


  test-plan-and-deploy-to-dev:
    unless: << pipeline.parameters.GHA_Action >>
    jobs:
      - task-containerisation:
          access_key: AWS_ACCESS_KEY_ID_non_prod
          secret_key: AWS_SECRET_ACCESS_KEY_non_prod
          aws_account_id: AWS_ACCOUNT_ID_non_prod
          filters:
            branches:
              ignore: main
      - test:
          filters:
            branches:
              ignore: main
      - lint-python:
          filters:
            branches:
              ignore: main
      - lint-terraform:
          filters:
            branches:
              ignore: main
      - lint-docstrings:
          filters:
            branches:
              ignore: main
      - terraform-plan:
          access_key: AWS_ACCESS_KEY_ID_non_prod
          secret_key: AWS_SECRET_ACCESS_KEY_non_prod
          aws_account: "non_prod"
          requires:
            - lint-terraform
            - test
            - task-containerisation
      - plan-approval: # TODO remove this before merge
          type: approval
          requires:
            - terraform-plan
      - terraform-apply:
          access_key: AWS_ACCESS_KEY_ID_non_prod
          secret_key: AWS_SECRET_ACCESS_KEY_non_prod
          aws_account: "non_prod"
          requires:
            - terraform-plan
      - deploy-dependencies:
          access_key: AWS_ACCESS_KEY_ID_non_prod
          secret_key: AWS_SECRET_ACCESS_KEY_non_prod
          requires:
            - terraform-apply
      - copy-default-data:
          requires:
            - terraform-apply

  delete-development-environment:
    when: << pipeline.parameters.GHA_Action >>
    jobs:
      - terraform-destroy:
          aws_account: "non_prod"
