version: 2.1

parameters:
  GHA_Actor:
    type: string
    default: ""
  GHA_Action:
    type: string
    default: ""
  GHA_Event:
    type: string
    default: ""
  GHA_Meta:
    type: string
    default: ""
  run-build-dataset-rebuild:
    type: boolean
    default: false
  run-check-dataset-equality:
    type: boolean
    default: false

orbs:
  aws-s3: circleci/aws-s3@3.0
  aws-cli: circleci/aws-cli@5.4.1

jobs:
  lambda-function-containerisation:
    docker:
      - image: docker:latest
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - aws-cli/setup:
          region: "eu-west-2"
      - run:
          name: Conditionally Build Docker Image
          command: |
            if ! [ git diff --quiet HEAD~1 -- "lambdas/create_dataset_snapshot" ] || [ $(aws ecr describe-images --registry-id  ${AWS_ACCOUNT_ID} --repository-name lambda/create-snapshot --query "imageDetails[?imageTags[0]=='$CIRCLE_BRANCH']") = "[]" ]; then
              echo "There are changes in the create dataset snapshot container. Rebuilding and pushing to ECR."
              docker buildx build  --platform linux/amd64 -t latest --no-cache --pull -f lambdas/create_dataset_snapshot/Dockerfile .
              
              # Authenticate Docker to ECR
              docker login -u AWS -p $(aws ecr get-login-password --region eu-west-2) ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com
              
              # Push the image
              docker tag latest ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com/lambda/create-snapshot:${CIRCLE_BRANCH}
              docker push ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com/lambda/create-snapshot:${CIRCLE_BRANCH}
            else
              echo "There are no changes in the create dataset snapshot container. Skipping."
            fi
          
            if ! [ git diff --quiet HEAD~1 -- "lambdas/check_dataset_equality" ] || [ $(aws ecr describe-images --registry-id  ${AWS_ACCOUNT_ID} --repository-name lambda/create-snapshot --query "imageDetails[?imageTags[0]=='$CIRCLE_BRANCH']") = "[]" ]; then
              echo "There are changes in the check dataset equality container. Rebuilding and pushing to ECR."
              docker buildx build  --platform linux/amd64 -t latest --no-cache --pull -f lambdas/check_dataset_equality/Dockerfile .

              # Authenticate Docker to ECR
              docker login -u AWS -p $(aws ecr get-login-password --region eu-west-2) ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com

              # Push the image
              docker tag latest ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com/lambda/check-datasets-equal:${CIRCLE_BRANCH}
              docker push ${AWS_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com/lambda/check-datasets-equal:${CIRCLE_BRANCH}
            else
              echo "There are no changes in the check dataset equality container. Skipping."
            fi

  build-check-equality:
    docker:
      - image: docker:latest
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - aws-cli/setup:
          region: "eu-west-2"
      - run:
          name: Conditionally Build Docker Image
          command: |
            

  test:
    docker:
      - image: circleci/python:3.9.6
    resource_class: small

    steps:
      - checkout

      - restore_cache:
          keys:
            - v1.1-deps-{{ checksum "Pipfile.lock" }}

      - run:
          name: Install system packages
          command: |
            sudo apt-get update
            sudo apt-get install -y default-jre-headless

      - run:
          name: install dependencies
          command: |
            sudo pip install pipenv
            pipenv install --dev

      - save_cache:
          key: v1.1-deps-{{ checksum "Pipfile.lock" }}
          paths:
            - ~/.local/share/virtualenvs

      - run:
          name: run unit tests
          command: |
            pipenv run python -m unittest discover -s . -p "test_*.py"

      - run:
          name: run integration tests
          command: |
            pipenv run python -m unittest discover tests/integration



  lint-python:
    docker:
      - image: circleci/python:3.9.6
    resource_class: small
    steps:
      - checkout
      - run:
          name: Install black
          command: pip install black==23.7.0
      - run:
          name: Check Python fomatting
          command: black . --check

  lint-docstrings:
    docker:
      - image: circleci/python:3.9.6
    resource_class: small
    steps:
      - checkout
      - run:
          name: Install pydoclint
          command: pip install pydoclint==0.5.9
      - run:
          name: Check docstring fomatting
          command: pydoclint --style=google --quiet .

  lint-terraform:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.2.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: Check terraform fomatting
          command: terraform fmt -check -recursive -diff

  terraform-plan:
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.2.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: terraform plan
          command: |
            cd terraform/pipeline
            terraform init -input=false
            terraform workspace select ${CIRCLE_BRANCH} || terraform workspace new ${CIRCLE_BRANCH}
            terraform plan -out tfapply
      - store_artifacts:
          path: terraform/pipeline/tfapply
      - persist_to_workspace:
          root: .
          paths:
            - terraform/pipeline/tfapply
            - terraform/pipeline/.*
            - .

  terraform-apply:
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.2.1
    resource_class: small
    steps:
      - checkout
      - attach_workspace:
          at: .

      - run:
          name: terraform apply
          command: |
            cd terraform/pipeline
            terraform apply -auto-approve tfapply

      - run:
          name: save pipeline resources bucket name
          command: |
            cd terraform/pipeline
            echo $(terraform output --raw pipeline_resources_bucket_name) > pipeline_resources_bucket_name

      - run:
          name: save datasets bucket name
          command: |
            cd terraform/pipeline
            echo $(terraform output --raw datasets_bucket_name) > datasets_bucket_name

      - persist_to_workspace:
          root: .
          paths:
            - ./terraform/pipeline/pipeline_resources_bucket_name
            - ./terraform/pipeline/datasets_bucket_name

  deploy-dependencies:
    docker:
      - image: circleci/python:3.9.6
    resource_class: small
    steps:
      - attach_workspace:
          at: .

      - run:
          name: package utils, schemas, projects and environment
          command: |
            mkdir dependencies && zip -r dependencies/dependencies.zip utils schemas projects environment

      - run:
          name: Download Deequ jar
          command: |
            wget https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.3/deequ-2.0.7-spark-3.3.jar -O dependencies/deequ-2.0.7-spark-3.3.jar

      - run:
          name: Package PyDeequ
          command: |
            pip install -t ./target pydeequ==1.4.0
            cd target && zip -r pydeequ-1.4.0.zip pydeequ && cd ..
            mv target/pydeequ-1.4.0.zip dependencies/pydeequ-1.4.0.zip
            rm -rf target

      - aws-s3/sync:
          aws-region: AWS_REGION
          from: dependencies
          to: "s3://$(cat terraform/pipeline/pipeline_resources_bucket_name)/dependencies"

  copy-default-data:
    docker:
      - image: cimg/base:current
    resource_class: small
    steps:
      - attach_workspace:
          at: .

      - aws-s3/sync:
          aws-region: AWS_REGION
          from: "s3://sfc-default-datasets/"
          to: "s3://$(cat terraform/pipeline/datasets_bucket_name)/"

      - aws-s3/sync:
          aws-region: AWS_REGION
          from: "s3://sfc-main-pipeline-resources/models/"
          to: "s3://$(cat terraform/pipeline/pipeline_resources_bucket_name)/models/"

  terraform-destroy:
    working_directory: /tmp/project
    docker:
      - image: docker.mirror.hashicorp.services/hashicorp/terraform:1.2.1
    resource_class: small
    steps:
      - checkout
      - run:
          name: terraform destroy
          command: |
            cd terraform/pipeline
            terraform init -input=false
            terraform workspace select << pipeline.parameters.GHA_Meta >>
            terraform destroy -auto-approve
      - run:
          name: delete workspace
          command: |
            cd terraform/pipeline
            terraform workspace select default
            terraform workspace delete << pipeline.parameters.GHA_Meta >>


workflows:
  version: 2

  test-plan-approve-and-deploy-to-main:
    unless: << pipeline.parameters.GHA_Action >>
    jobs:
      - lambda-function-containerisation:
          filters:
            branches:
              only: main
      - test:
          filters:
            branches:
              only: main
      - terraform-plan:
          requires:
            - test
            - lambda-function-containerisation
      - plan-approval:
          type: approval
          requires:
            - terraform-plan
      - terraform-apply:
          requires:
            - plan-approval
      - deploy-dependencies:
          requires:
            - terraform-apply


  test-plan-and-deploy-to-dev:
    unless: << pipeline.parameters.GHA_Action >>
    jobs:
      - lambda-function-containerisation:
          filters:
            branches:
              ignore: main
#      - test:
#          filters:
#            branches:
#              ignore: main
#      - lint-python:
#          filters:
#            branches:
#              ignore: main
#      - lint-terraform:
#          filters:
#            branches:
#              ignore: main
#      - lint-docstrings:
#          filters:
#            branches:
#              ignore: main
#      - terraform-plan:
#          requires:
#            - lint-terraform
#            - test
#            - lambda-function-containerisation
#      - terraform-apply:
#          requires:
#            - terraform-plan
#      - deploy-dependencies:
#          requires:
#            - terraform-apply
#      - copy-default-data:
#          requires:
#            - terraform-apply

  delete-development-environment:
    when: << pipeline.parameters.GHA_Action >>
    jobs:
      - terraform-destroy
